{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import io\n",
        "import requests\n",
        "\n",
        "# Try to import the dataset using a different approach\n",
        "try:\n",
        "    # Method 1: Try with different headers to mimic a browser\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    url = 'https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv'\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        dataset = pd.read_csv(io.StringIO(response.text))\n",
        "        print(\"Successfully loaded the dataset using requests!\")\n",
        "    else:\n",
        "        raise Exception(f\"Failed to load data: HTTP {response.status_code}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset from URL: {e}\")\n",
        "    print(\"Creating a sample dataset based on the expected structure...\")\n",
        "\n",
        "    # Create a sample dataset with similar structure and distributions\n",
        "    # This is a fallback in case the URL doesn't work\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1338  # Approximate size of the original dataset\n",
        "\n",
        "    # Generate synthetic data with realistic distributions\n",
        "    age = np.random.randint(18, 65, n_samples)\n",
        "    sex = np.random.choice(['male', 'female'], n_samples)\n",
        "    bmi = np.random.normal(30, 6, n_samples).clip(15, 50)\n",
        "    children = np.random.randint(0, 6, n_samples)\n",
        "    smoker = np.random.choice(['yes', 'no'], n_samples, p=[0.2, 0.8])\n",
        "    region = np.random.choice(['northeast', 'northwest', 'southeast', 'southwest'], n_samples)\n",
        "\n",
        "    # Generate expenses with realistic correlations\n",
        "    base_expenses = 5000 + 100 * (age - 18) + 500 * (bmi - 20)\n",
        "    smoker_effect = np.where(smoker == 'yes', 20000, 0)\n",
        "    children_effect = 2000 * children\n",
        "    random_effect = np.random.normal(0, 4000, n_samples)\n",
        "    expenses = (base_expenses + smoker_effect + children_effect + random_effect).clip(1000, 60000)\n",
        "\n",
        "    # Create the DataFrame\n",
        "    data = {\n",
        "        'age': age,\n",
        "        'sex': sex,\n",
        "        'bmi': bmi,\n",
        "        'children': children,\n",
        "        'smoker': smoker,\n",
        "        'region': region,\n",
        "        'expenses': expenses\n",
        "    }\n",
        "    dataset = pd.DataFrame(data)\n",
        "    print(\"Created synthetic dataset for demonstration purposes.\")\n",
        "\n",
        "# Display the dataset\n",
        "print(\"\\nDataset head:\")\n",
        "print(dataset.head())\n",
        "\n",
        "# Check the shape of the dataset\n",
        "print(f\"\\nDataset shape: {dataset.shape}\")\n",
        "\n",
        "# Check data types and missing values\n",
        "print(\"\\nData info:\")\n",
        "dataset.info()\n",
        "\n",
        "# Statistical summary\n",
        "print(\"\\nStatistical summary:\")\n",
        "print(dataset.describe())\n",
        "\n",
        "# Check unique values for categorical columns\n",
        "print(\"\\nUnique values in categorical columns:\")\n",
        "for col in dataset.select_dtypes(include=['object']).columns:\n",
        "    print(f\"{col}: {dataset[col].unique()}\")\n",
        "\n",
        "# Create a copy of the dataset\n",
        "processed_dataset = dataset.copy()\n",
        "\n",
        "# Convert sex to numerical (0 for female, 1 for male)\n",
        "processed_dataset['sex'] = processed_dataset['sex'].map({'female': 0, 'male': 1})\n",
        "\n",
        "# Convert smoker to numerical (0 for no, 1 for yes)\n",
        "processed_dataset['smoker'] = processed_dataset['smoker'].map({'no': 0, 'yes': 1})\n",
        "\n",
        "# One-hot encode the 'region' column\n",
        "processed_dataset = pd.get_dummies(processed_dataset, columns=['region'], prefix='region')\n",
        "\n",
        "print(\"\\nProcessed dataset head:\")\n",
        "print(processed_dataset.head())\n",
        "\n",
        "# Split the data into train and test sets (80% train, 20% test)\n",
        "train_dataset = processed_dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = processed_dataset.drop(train_dataset.index)\n",
        "\n",
        "# Pop off the \"expenses\" column to create labels\n",
        "train_labels = train_dataset.pop('expenses')\n",
        "test_labels = test_dataset.pop('expenses')\n",
        "\n",
        "# Check the shapes\n",
        "print(f\"\\nTraining data shape: {train_dataset.shape}\")\n",
        "print(f\"Training labels shape: {train_labels.shape}\")\n",
        "print(f\"Testing data shape: {test_dataset.shape}\")\n",
        "print(f\"Testing labels shape: {test_labels.shape}\")\n",
        "\n",
        "# Normalize the data\n",
        "train_stats = train_dataset.describe()\n",
        "train_stats = train_stats.transpose()\n",
        "\n",
        "def norm(x):\n",
        "    return (x - train_stats['mean']) / train_stats['std']\n",
        "\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)\n",
        "\n",
        "print(\"\\nNormalized training data head:\")\n",
        "print(normed_train_data.head())\n",
        "\n",
        "# Build the model\n",
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=[len(train_dataset.columns)]),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "    model.compile(loss='mse',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['mae', 'mse'])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# Display model summary\n",
        "print(\"\\nModel summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Train the model with early stopping\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "print(\"\\nTraining the model...\")\n",
        "history = model.fit(\n",
        "    normed_train_data, train_labels,\n",
        "    epochs=100,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Abs Error [expenses]')\n",
        "plt.plot(hist['epoch'], hist['mae'], label='Train Error')\n",
        "plt.plot(hist['epoch'], hist['val_mae'], label='Val Error')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Square Error [$expenses^2$]')\n",
        "plt.plot(hist['epoch'], hist['mse'], label='Train Error')\n",
        "plt.plot(hist['epoch'], hist['val_mse'], label='Val Error')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "print(f\"\\nTesting set Mean Abs Error: ${mae:0.2f}\")\n",
        "\n",
        "# Make predictions\n",
        "test_predictions = model.predict(normed_test_data).flatten()\n",
        "\n",
        "# Plot predictions vs actual\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [expenses]')\n",
        "plt.ylabel('Predictions [expenses]')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim([0, plt.xlim()[1]])\n",
        "plt.ylim([0, plt.ylim()[1]])\n",
        "plt.plot([-100, 5000], [-100, 5000])\n",
        "plt.title('Model Predictions vs Actual Values')\n",
        "plt.show()\n",
        "\n",
        "# Plot error distribution\n",
        "error = test_predictions - test_labels\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(error, bins=25)\n",
        "plt.xlabel(\"Prediction Error [expenses]\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title('Error Distribution')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean Absolute Error: ${np.mean(np.abs(error)):0.2f}\")\n",
        "\n",
        "# If the model doesn't achieve a MAE under $3500, try an improved model\n",
        "if mae > 3500:\n",
        "    print(\"\\nTrying an improved model...\")\n",
        "\n",
        "    # Build a more complex model with additional layers and neurons\n",
        "    def build_improved_model():\n",
        "        model = keras.Sequential([\n",
        "            layers.Dense(128, activation='relu', input_shape=[len(train_dataset.columns)]),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "        model.compile(loss='mse',\n",
        "                    optimizer=optimizer,\n",
        "                    metrics=['mae', 'mse'])\n",
        "        return model\n",
        "\n",
        "    improved_model = build_improved_model()\n",
        "    print(\"\\nImproved model summary:\")\n",
        "    improved_model.summary()\n",
        "\n",
        "    # Train with more epochs and patience\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "    print(\"\\nTraining the improved model...\")\n",
        "    history = improved_model.fit(\n",
        "        normed_train_data, train_labels,\n",
        "        epochs=200,\n",
        "        validation_split=0.2,\n",
        "        verbose=1,\n",
        "        callbacks=[early_stop, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # Evaluate the improved model\n",
        "    loss, mae, mse = improved_model.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "    print(f\"\\nImproved model - Testing set Mean Abs Error: ${mae:0.2f}\")\n",
        "\n",
        "    # Make predictions with the improved model\n",
        "    test_predictions = improved_model.predict(normed_test_data).flatten()\n",
        "\n",
        "    # Plot predictions vs actual for improved model\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(test_labels, test_predictions)\n",
        "    plt.xlabel('True Values [expenses]')\n",
        "    plt.ylabel('Predictions [expenses]')\n",
        "    plt.axis('equal')\n",
        "    plt.axis('square')\n",
        "    plt.xlim([0, plt.xlim()[1]])\n",
        "    plt.ylim([0, plt.ylim()[1]])\n",
        "    plt.plot([-100, 5000], [-100, 5000])\n",
        "    plt.title('Improved Model Predictions vs Actual Values')\n",
        "    plt.show()\n",
        "\n",
        "    error = test_predictions - test_labels\n",
        "    print(f\"Improved model - Mean Absolute Error: ${np.mean(np.abs(error)):0.2f}\")"
      ],
      "metadata": {
        "id": "fhJnC5yrzVsX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "fcc_predict_health_costs_with_regression.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}